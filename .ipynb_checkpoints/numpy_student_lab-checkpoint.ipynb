{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THsE3ikMhmUQ"
   },
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8e2llPlAhiFx"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QadE19phhwQP"
   },
   "source": [
    "Section 1 — ndarray Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sh8N_P9Qhx_N"
   },
   "source": [
    "Task 1.1: Array Creation & Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fA0dNmozhpEJ"
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Create a 1D array with values 0 to 99 (no loops)\n",
    "\n",
    "# HINT:\n",
    "# - Use np.arange\n",
    "arr_1d = np.arange(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "llCJGu2Qh1mu"
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Reshape arr_1d into a (10, 10) array\n",
    "\n",
    "# HINT:\n",
    "# - reshape does NOT copy data\n",
    "arr_2d = arr_1d.reshape(10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "J2AUBC4jh32D"
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Create a 3D array of shape (4, 5, 3)\n",
    "\n",
    "# HINT:\n",
    "# - Total elements must match\n",
    "\n",
    "arr_3d = np.arange(4 * 5 * 3).reshape(4, 5, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4] [95 96 97 98 99]\n",
      "(10, 10) [0 1 2 3 4]\n",
      "(4, 5, 3) [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "assert arr_1d.shape == (100,)\n",
    "assert arr_2d.shape == (10, 10)\n",
    "assert arr_3d.shape == (4, 5, 3)\n",
    "\n",
    "print(arr_1d[:5], arr_1d[-5:])\n",
    "print(arr_2d.shape, arr_2d[0, :5])\n",
    "print(arr_3d.shape, arr_3d[0, 0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cI5EhQ0IiEu8"
   },
   "source": [
    "**Explain:**\n",
    "- What does `.shape` represent?\n",
    "\n",
    "**Answer:** * .shape represents the dimensions of a data structure, usually a NumPy array. It tells you how many elements exist along each axis (for example, (rows, columns) for a 2D array), which helps you understand the structure of the data and ensures operations like broadcasting and matrix multiplication work correctly.\n",
    "\n",
    "- Why does contiguous memory matter?\n",
    "\n",
    "**Answer:*** Contiguous memory matters because when data is stored next to each other in memory, the CPU can access it much faster due to better cache usage and fewer memory jumps. Many low-level optimizations (and NumPy operations) assume contiguous blocks, so contiguous arrays enable faster computation and simpler vectorized operations, while non-contiguous memory can slow things down or require copying.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0d5b05eiJ9W"
   },
   "source": [
    "Section 1.2 — dtype & Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "cK_OEpKjh6KD"
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Create two arrays with same values but different dtypes\n",
    "base = np.arange(1000)\n",
    "arr_int = base.astype(np.int64)\n",
    "arr_float = base.astype(np.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "J25XhniciNmu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64 nbytes: 8000\n",
      "float64 nbytes: 8000\n",
      "dtype: int64 float64\n"
     ]
    }
   ],
   "source": [
    "# TODO:\n",
    "# Compare memory usage\n",
    "\n",
    "# HINT:\n",
    "# - Use .nbytes\n",
    "print('int64 nbytes:', arr_int.nbytes)\n",
    "print('float64 nbytes:', arr_float.nbytes)\n",
    "print('dtype:', arr_int.dtype, arr_float.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Doas6bL-iX8k"
   },
   "source": [
    "**Interview Question:**  \n",
    "Why does dtype selection matter in large ML pipelines?\n",
    "\n",
    "**Answer:** `dtype` matters because it controls memory, speed, and numerical precision. Smaller or hardware-friendly dtypes reduce memory usage and speed up computation, while poor choices can cause precision loss or overflow, breaking training at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4tHMXhCieQi"
   },
   "source": [
    "Section 2 — Indexing, Views & Copies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKv_zCtQihrL"
   },
   "source": [
    "Task 2.1: Views vs Copies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "UFoMJJr1iRPn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_slice:\n",
      " [[-999    1]\n",
      " [   4    5]\n",
      " [   8    9]\n",
      " [  12   13]\n",
      " [  16   17]]\n",
      "A changed too (view):\n",
      " [[-999    1]\n",
      " [   2    3]\n",
      " [   4    5]\n",
      " [   6    7]\n",
      " [   8    9]\n",
      " [  10   11]\n",
      " [  12   13]\n",
      " [  14   15]\n",
      " [  16   17]\n",
      " [  18   19]]\n"
     ]
    }
   ],
   "source": [
    "# TODO:\n",
    "# Create a 2D array and slice every alternate row\n",
    "\n",
    "# HINT:\n",
    "# - Use slicing, not fancy indexing\n",
    "\n",
    "A = np.arange(20).reshape(10, 2)\n",
    "A_slice = A[::2]\n",
    "A_slice[0, 0] = -999\n",
    "print('A_slice:\\n', A_slice)\n",
    "print('A changed too (view):\\n', A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "RZwW0qeLiYpP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fancy indexing copy changed: 123456\n",
      "Original A unchanged at [0,0]: -999\n"
     ]
    }
   ],
   "source": [
    "# TODO:\n",
    "# Modify A_slice and observe A\n",
    "\n",
    "A_copy = A[[0, 2, 4]]\n",
    "A_copy[0, 0] = 123456\n",
    "print('Fancy indexing copy changed:', A_copy[0, 0])\n",
    "print('Original A unchanged at [0,0]:', A[0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GttQjx7ipDQ"
   },
   "source": [
    "Explain:\n",
    "- Why did the original array change (or not)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X6W2OiBDiuaH"
   },
   "source": [
    "Section 2.2 — Boolean Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "WOaETRXBimsR"
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Create random array of size 1000\n",
    "\n",
    "X = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "1lqB6hi1iw1N"
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Extract values greater than mean\n",
    "\n",
    "# HINT:\n",
    "# - Mean first\n",
    "# - Boolean mask\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "uhI9Mq9hiy-e"
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Replace negative values with 0 (no loops)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: -0.04802827676298692\n",
      "above_mean count: 488\n",
      "negatives after replace: 0\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "X = rng.standard_normal(1000)\n",
    "m = X.mean()\n",
    "above_mean = X[X > m]\n",
    "\n",
    "X_nonneg = X.copy()\n",
    "X_nonneg[X_nonneg < 0] = 0\n",
    "\n",
    "print('mean:', m)\n",
    "print('above_mean count:', above_mean.size)\n",
    "print('negatives after replace:', (X_nonneg < 0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IiPyR8Xni8M8"
   },
   "source": [
    "Section 3 — Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ZzNLC4zi-W1"
   },
   "source": [
    "Task 3.1: Broadcasting Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "vBzCyUtdi2AN"
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Create A (1000, 50) and b (50,)\n",
    "\n",
    "A = ...\n",
    "b = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "_gwUX5izjBh-"
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Add b to each row of A\n",
    "\n",
    "# HINT:\n",
    "# - No reshape required\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ZlPV3LMFjDf0"
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Normalize each row of A\n",
    "\n",
    "# HINT:\n",
    "# - Axis matters\n",
    "# - Keep dimensions in mind\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 50) (1000, 50)\n",
      "row means ~0: True\n",
      "row stds ~1: True\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng(1)\n",
    "A = rng.standard_normal((1000, 50))\n",
    "b = rng.standard_normal((50,))\n",
    "\n",
    "A_plus = A + b\n",
    "\n",
    "# Normalize each row: (row - mean(row)) / std(row)\n",
    "row_mean = A.mean(axis=1, keepdims=True)\n",
    "row_std = A.std(axis=1, keepdims=True)\n",
    "A_norm = (A - row_mean) / row_std\n",
    "\n",
    "print(A_plus.shape, A_norm.shape)\n",
    "print('row means ~0:', np.allclose(A_norm.mean(axis=1), 0, atol=1e-6))\n",
    "print('row stds ~1:', np.allclose(A_norm.std(axis=1), 1, atol=1e-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "damGC04kjJ4C"
   },
   "source": [
    "Explain broadcasting step-by-step.\n",
    "\n",
    "Answer: Broadcasting is the process by which NumPy or similar libraries allow arrays of different shapes to work together without copying data. It works by aligning the shapes from the right and checking each dimension for compatibility: the dimensions must either be equal or one of them must be 1. If an array has fewer dimensions, the missing leading dimensions are treated as size 1. The output shape is determined by taking the maximum size along each axis, and any dimension with size 1 is logically “stretched” to match the other array. This allows efficient element-wise operations while keeping memory usage low.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZJZS8x6jMSa"
   },
   "source": [
    "Section 3.2 — Broadcasting Trap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "zBChDtpejHN6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected broadcasting error: operands could not be broadcast together with shapes (1000,50) (1000,) \n",
      "Fixed shape: (1000, 50)\n"
     ]
    }
   ],
   "source": [
    "# TODO:\n",
    "# Intentionally trigger a broadcasting error\n",
    "# Then fix it\n",
    "col = np.arange(1000)  # shape (1000,)\n",
    "try:\n",
    "    _ = A - col\n",
    "except ValueError as e:\n",
    "    print('Expected broadcasting error:', e)\n",
    "\n",
    "# Fix by making it a column vector\n",
    "col2 = col.reshape(-1, 1)\n",
    "A_fixed = A - col2\n",
    "print('Fixed shape:', A_fixed.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87Hg6pHcjUz_"
   },
   "source": [
    "What was wrong with the original shapes?\n",
    "\n",
    "Answer:\n",
    "The problem was that the shapes were not broadcast-compatible.\n",
    "`A` is a 2D array with shape `(1000, n)` (rows × columns), while `col` has shape `(1000,)`, which NumPy treats as `(1, 1000)` when aligning dimensions from the right. That leads to a mismatch: NumPy tries to align `n` with `1000`, which fails unless `n == 1000`. By reshaping `col` to `(1000, 1)`, its shape aligns correctly with `A`—the `1000` rows match, and the `1` column can be broadcast across all columns—so the subtraction works.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7lJkIZKwjbic"
   },
   "source": [
    "Section 4 — Vectorization vs Loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-ku-91djdb5"
   },
   "source": [
    "Task 4.1: Loop vs Vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "9HEd9iD7jRkx"
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Create large array X of size 1,000,000\n",
    "\n",
    "X = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "2mwCsbQIjb6C"
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Normalize using Python loop\n",
    "\n",
    "# HINT:\n",
    "# - Time it\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "VnBPlOLPjjEQ"
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Normalize using vectorization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop seconds: 0.1559\n",
      "vec seconds : 0.0067\n",
      "speedup     : 23.4 x\n",
      "allclose    : True\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng(2)\n",
    "X = rng.standard_normal(1_000_000)\n",
    "mu = X.mean()\n",
    "sigma = X.std()\n",
    "\n",
    "# Loop\n",
    "t0 = time.time()\n",
    "out_loop = np.empty_like(X)\n",
    "for i in range(X.shape[0]):\n",
    "    out_loop[i] = (X[i] - mu) / sigma\n",
    "t_loop = time.time() - t0\n",
    "\n",
    "# Vectorized\n",
    "t1 = time.time()\n",
    "out_vec = (X - mu) / sigma\n",
    "t_vec = time.time() - t1\n",
    "\n",
    "print('loop seconds:', round(t_loop, 4))\n",
    "print('vec seconds :', round(t_vec, 4))\n",
    "print('speedup     :', round(t_loop / max(t_vec, 1e-12), 1), 'x')\n",
    "print('allclose    :', np.allclose(out_loop, out_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4JkKYiujqns"
   },
   "source": [
    "Why is vectorization faster?\n",
    "\n",
    "Answer:Vectorization is faster because it moves work out of Python loops and into optimized, low-level C/Fortran code that operates on whole arrays at once. This reduces Python interpreter overhead, improves CPU cache usage, and enables hardware-level optimizations like SIMD instructions. As a result, fewer instructions are executed in Python, and the CPU processes data more efficiently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVuPkVV8jtLF"
   },
   "source": [
    "Task 4.2: Pairwise Distance (FAANG Classic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "XISh6eh3joJ7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 200)\n",
      "diag ~0: True\n",
      "symmetric: True\n"
     ]
    }
   ],
   "source": [
    "# TODO:\n",
    "# Compute pairwise Euclidean distance matrix without loops\n",
    "\n",
    "# HINT:\n",
    "# - Use (x - y)^2 expansion\n",
    "# - Broadcasting is key\n",
    "\n",
    "def pairwise_distance(X):\n",
    "    # ||x-y||^2 = ||x||^2 + ||y||^2 - 2 x·y\n",
    "    # X: (n, d)\n",
    "    sq = np.sum(X * X, axis=1, keepdims=True)  # (n,1)\n",
    "    dist2 = sq + sq.T - 2 * (X @ X.T)\n",
    "    dist2 = np.maximum(dist2, 0.0)  # numerical guard\n",
    "    return np.sqrt(dist2)\n",
    "\n",
    "rng = np.random.default_rng(3)\n",
    "Xsmall = rng.standard_normal((200, 10))\n",
    "D = pairwise_distance(Xsmall)\n",
    "print(D.shape)\n",
    "print('diag ~0:', np.allclose(np.diag(D), 0, atol=1e-7))\n",
    "print('symmetric:', np.allclose(D, D.T, atol=1e-7))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7rZh53cjy9I"
   },
   "source": [
    "Section 5 — Numerical Stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQLltnsQjzs-"
   },
   "source": [
    "Task 5.1: Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "thv3qP03jwbC"
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Implement naive softmax\n",
    "\n",
    "def softmax_naive(X):\n",
    "    ex = np.exp(X)\n",
    "    return ex / ex.sum(axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "o9JGLgIOj6K2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive: [[nan nan nan]]\n",
      "stable: [[0.09003057 0.24472847 0.66524096]]\n",
      "rowsum: [1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x8/3_z8cvr917b47x_2vb82b2_h0000gn/T/ipykernel_55780/3571301798.py:5: RuntimeWarning: overflow encountered in exp\n",
      "  ex = np.exp(X)\n",
      "/var/folders/x8/3_z8cvr917b47x_2vb82b2_h0000gn/T/ipykernel_55780/3571301798.py:6: RuntimeWarning: invalid value encountered in divide\n",
      "  return ex / ex.sum(axis=1, keepdims=True)\n"
     ]
    }
   ],
   "source": [
    "# TODO:\n",
    "# Fix numerical instability\n",
    "\n",
    "# HINT:\n",
    "# - Subtract max per row\n",
    "\n",
    "def softmax_stable(X):\n",
    "    Z = X - X.max(axis=1, keepdims=True)\n",
    "    ex = np.exp(Z)\n",
    "    return ex / ex.sum(axis=1, keepdims=True)\n",
    "\n",
    "Xbig = np.array([[1000.0, 1001.0, 1002.0]])\n",
    "try:\n",
    "    print('naive:', softmax_naive(Xbig))\n",
    "except FloatingPointError as e:\n",
    "    print('overflow:', e)\n",
    "\n",
    "print('stable:', softmax_stable(Xbig))\n",
    "print('rowsum:', softmax_stable(Xbig).sum(axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zlzVyosFkCwi"
   },
   "source": [
    "Why does subtracting max work?\n",
    "\n",
    "Answer: Subtracting the maximum works because it doesn’t change relative differences, only the scale. In operations like softmax, all values are shifted by the same constant, so the final probabilities remain the same mathematically. However, this shift prevents very large exponentials, which could otherwise cause overflow and numerical instability. By keeping values closer to zero, computations stay stable and accurate without affecting the result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hR3jiBxbkFSb"
   },
   "source": [
    "Section 6 — Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rg9_HtukkHCg"
   },
   "source": [
    "Task 6.1: Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "EpheXo3gkAuU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C shape: (3, 2)\n",
      "Expected invalid matmul: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 4)\n",
      "np.dot(A, B) == A@B: True\n",
      "np.matmul(A, B) == A@B: True\n"
     ]
    }
   ],
   "source": [
    "# TODO:\n",
    "# Try valid and invalid matrix multiplications\n",
    "A = np.random.default_rng(4).standard_normal((3, 4))\n",
    "B = np.random.default_rng(5).standard_normal((4, 2))\n",
    "C = A @ B\n",
    "print('C shape:', C.shape)\n",
    "\n",
    "try:\n",
    "    _ = A @ A\n",
    "except ValueError as e:\n",
    "    print('Expected invalid matmul:', e)\n",
    "\n",
    "print('np.dot(A, B) == A@B:', np.allclose(np.dot(A, B), C))\n",
    "print('np.matmul(A, B) == A@B:', np.allclose(np.matmul(A, B), C))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVfOrat1kMr8"
   },
   "source": [
    "Explain difference between dot, @, and matmul.\n",
    "\n",
    "Answer: `dot`, `@`, and `matmul` are related but behave differently depending on array dimensions. `np.dot` is overloaded: for 1D arrays it computes the inner product, for 2D arrays it performs matrix multiplication, but for higher dimensions its behavior can be confusing and less consistent. `@` is the matrix multiplication operator introduced in Python for clarity; it follows strict matrix-multiplication rules and is equivalent to `np.matmul`. `np.matmul` is the explicit function version of `@` and supports broadcasting over batch dimensions for higher-rank arrays, which makes it the preferred and safest choice in modern NumPy and ML code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HU3JZWorkPpg"
   },
   "source": [
    "Task 6.2: Solving Linear Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "NnQSyMMAkJjP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "residual: 1.3472180148700993e-15\n"
     ]
    }
   ],
   "source": [
    "# TODO:\n",
    "# Solve Ax = b and verify solution\n",
    "rng = np.random.default_rng(6)\n",
    "A = rng.standard_normal((5, 5))\n",
    "b = rng.standard_normal((5,))\n",
    "x = np.linalg.solve(A, b)\n",
    "resid = np.linalg.norm(A @ x - b)\n",
    "print('residual:', resid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7QeS-ldakXCR"
   },
   "source": [
    "Section 7 — Performance & Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4x_u1MHTkSiM"
   },
   "source": [
    "Task 7.1: In-Place Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "eBjd0sjkkR-J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out-of-place seconds: 0.003\n",
      "in-place seconds    : 0.0017\n",
      "out-of-place allocs new array (Z is new): True\n"
     ]
    }
   ],
   "source": [
    "# TODO:\n",
    "# Compare in-place vs out-of-place operations\n",
    "\n",
    "X = np.random.default_rng(7).standard_normal(2_000_000)\n",
    "Y = X.copy()\n",
    "\n",
    "t0 = time.time(); Z = X + 1.0; t_out = time.time() - t0\n",
    "t1 = time.time(); Y += 1.0; t_in = time.time() - t1\n",
    "\n",
    "print('out-of-place seconds:', round(t_out, 4))\n",
    "print('in-place seconds    :', round(t_in, 4))\n",
    "print('out-of-place allocs new array (Z is new):', Z is not X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4w93mTd4kfsa"
   },
   "source": [
    "Task 7.2: Strides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "6OYTZ5WYkcNM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A shape: (6, 4) strides: (32, 8)\n",
      "B shape: (6, 2) strides: (32, 16)\n"
     ]
    }
   ],
   "source": [
    "# TODO:\n",
    "# Inspect array strides and explain\n",
    "A = np.arange(24).reshape(6, 4)\n",
    "print('A shape:', A.shape, 'strides:', A.strides)\n",
    "B = A[:, ::2]\n",
    "print('B shape:', B.shape, 'strides:', B.strides)\n",
    "\n",
    "# Strides describe how many bytes to step in each dimension.\n",
    "# Non-contiguous views (like column slicing) can slow down some ops.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJS6dnBXkrMn"
   },
   "source": [
    "Section 8 — Mini Case Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "NZm_yBGrkjDG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C shape: (100, 100)\n",
      "W shape: (100, 5)\n",
      "Z shape: (10000, 5)\n"
     ]
    }
   ],
   "source": [
    "# TODO:\n",
    "# Given X (10000, 100):\n",
    "# - Normalize features\n",
    "# - Compute covariance\n",
    "# - Extract top-k eigenvectors\n",
    "rng = np.random.default_rng(8)\n",
    "X = rng.standard_normal((10_000, 100))\n",
    "\n",
    "# Normalize features (z-score per column)\n",
    "mu = X.mean(axis=0, keepdims=True)\n",
    "sigma = X.std(axis=0, keepdims=True)\n",
    "Xn = (X - mu) / sigma\n",
    "\n",
    "# Covariance (features x features)\n",
    "C = (Xn.T @ Xn) / (Xn.shape[0] - 1)\n",
    "\n",
    "# Top-k eigenvectors\n",
    "eigvals, eigvecs = np.linalg.eigh(C)\n",
    "k = 5\n",
    "top_idx = np.argsort(eigvals)[-k:][::-1]\n",
    "W = eigvecs[:, top_idx]  # (100, k)\n",
    "\n",
    "# Project\n",
    "Z = Xn @ W\n",
    "print('C shape:', C.shape)\n",
    "print('W shape:', W.shape)\n",
    "print('Z shape:', Z.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDa-rOw7kxZi"
   },
   "source": [
    "Explain each step and its ML relevance.\n",
    "\n",
    "Answer:\n",
    "* Generate data `X` (10000, 100): Think of this as 10,000 samples with 100 features each (a typical ML dataset matrix).\n",
    "\n",
    "* Normalize features (z-score):\n",
    "  `mu` and `sigma` compute per-feature mean/std, and `Xn = (X - mu)/sigma` makes each feature have ~0 mean and ~1 std.\n",
    "  ML relevance: prevents features with large scales from dominating distance-based models (kNN, k-means), gradients (logistic regression, neural nets), and especially PCA since covariance is scale-sensitive.\n",
    "\n",
    "* Compute covariance `C`:\n",
    "  `C = (Xn.T @ Xn) / (n-1)` produces a (100×100) matrix where entry `(i,j)` is how much feature *i* and feature *j* vary together.\n",
    "  **ML relevance:** captures the correlation structure of features; PCA is basically “find directions of maximum variance” from this matrix.\n",
    "\n",
    "* **Eigen-decomposition `eigh(C)`:**\n",
    "  Returns `eigvals` (variance explained by each principal direction) and `eigvecs` (the directions themselves). `eigh` is used because `C` is symmetric.\n",
    "  **ML relevance:** eigenvectors are principal components; eigenvalues tell you how important each component is.\n",
    "\n",
    "* **Pick top-k components:**\n",
    "  Sort eigenvalues descending, take the top 5 indices, and set `W = eigvecs[:, top_idx]` so `W` is **(100, 5)**.\n",
    "  **ML relevance:** keeps only the most informative directions → **dimensionality reduction**, noise reduction, faster models, less overfitting.\n",
    "\n",
    "* **Project to low-dim space `Z = Xn @ W`:**\n",
    "  Produces `Z` with shape **(10000, 5)**—each sample is now represented by 5 PCA features.\n",
    "  **ML relevance:** use `Z` for downstream tasks (classification/regression/clustering/visualization) with fewer dimensions while preserving as much variance as possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILzCnvoMk0S0"
   },
   "source": [
    "1. **Where did NumPy save memory?**\n",
    "\n",
    "* `mu` and `sigma` are kept as shape **(1, 100)** via `keepdims=True` (tiny) and broadcast across rows instead of being expanded to `(10000,100)`.\n",
    "* `col.reshape(-1,1)`-style reshapes are typically **views** (no copy) — same idea here: `X.T` is a view.\n",
    "  (But note: `Xn = (X - mu) / sigma` **does allocate** a full new `(10000,100)` array.)\n",
    "\n",
    "2. **Where did it avoid Python overhead?**\n",
    "\n",
    "* All the heavy work is done in optimized C/BLAS: `mean`, `std`, `@` (matrix multiplies), `eigh`, `argsort`, and `Xn @ W`.\n",
    "* No explicit Python `for` loops over rows/features.\n",
    "\n",
    "3. **Which operation would break at scale?**\n",
    "\n",
    "* Building the full covariance **`C` is O(d²)** memory/time, and **`np.linalg.eigh(C)` is ~O(d³)** time. As feature count `d` gets large (e.g., tens of thousands), eigen-decomposition becomes the bottleneck / infeasible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
